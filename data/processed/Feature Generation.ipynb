{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "### This is Mukund's attempt at creating different features. Each cell corresponds to a new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "This book will be processed: Dickens Bleak_House.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukund\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book will be processed: Dickens David_Copperfield.txt\n",
      "This book will be processed: Dickens Dombey_and_Son.txt\n",
      "This book will be processed: Dickens Great_Expectations.txt\n",
      "This book will be processed: Dickens Little_Dorrit.txt\n",
      "This book will be processed: Dickens Nicholas_Nickleby.txt\n",
      "This book will be processed: Dickens Oliver_Twist.txt\n",
      "This book will be processed: Dickens Our_Mutual_Friend.txt\n",
      "This book will be processed: Dickens The_Letters_of_Charles_Dickens.txt\n",
      "This book will be processed: Dickens The_Pickwick_Papers.txt\n",
      "Finished with Dickens's books!\n",
      "This book will be processed: Tolstoy Anna_Karenina.txt\n",
      "This book will be processed: Tolstoy Kingdom_of_God_is_Within_You.txt\n",
      "This book will be processed: Tolstoy Sevastopol.txt\n",
      "This book will be processed: Tolstoy The_Cossacks.txt\n",
      "This book will be processed: Tolstoy The_Kreutzer_Sonata_and_Other_Stories.txt\n",
      "This book will be processed: Tolstoy The_Resurrection.txt\n",
      "This book will be processed: Tolstoy War_And_Peace.txt\n",
      "Finished with Tolstoy's books!\n",
      "This book will be processed: Twain Adventures_of_Huckleberry_Finn.txt\n",
      "This book will be processed: Twain A_Connecticut_Yankee_in_King_Arthur's_Court.txt\n",
      "This book will be processed: Twain Life_on_the_Mississippi.txt\n",
      "This book will be processed: Twain Roughing_It.txt\n",
      "This book will be processed: Twain The_Adventures_Of_Tom_Sawyer.txt\n",
      "This book will be processed: Twain The_Innocents_Abroad.txt\n",
      "This book will be processed: Twain The_Prince_And_The_Pauper.txt\n",
      "This book will be processed: Twain The_Tragedy_of_Pudd'nhead_Wilson.txt\n",
      "Finished with Twain's books!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re #Regular expression operations\n",
    "import string\n",
    "import pandas as pd #To create a dataframe of data\n",
    "#NLTK is an interesting library that was used in a Kaggle kernel and helps with a bunch of NLP stuff\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords #for removing stopwords\n",
    "from collections import OrderedDict\n",
    "\n",
    "#######################################Remove stopwords from string##############################\n",
    "def remStopwordsFromStr(string1):\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #compiling all stopwords.\n",
    "    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n",
    "    return string2\n",
    "###########################End of Function#######################################################\n",
    "\n",
    "###########################Function for removing punctuations from string########################\n",
    "def remPuncFromStr(string1):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None\n",
    "    string2 = string1.translate(translation_table) #apply punctuation removal\n",
    "    return string2\n",
    "############################End of Function###############################################\n",
    "\n",
    "############################Most common proper noun in piece of text#############################\n",
    "def mostFreqProperNoun(tag_prefix, tagged_text):\n",
    "    #Let's get rid of some of the overly common proper nouns:\n",
    "    commPropNouns = [\"Mr\", \"Mrs\", \"Miss\", \"Ms\"]\n",
    "    \n",
    "    #Create an initial dict with the 4 most commonly occuring proper nouns and plural proper nouns\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                   if tag.startswith(tag_prefix))\n",
    "    fourMostCommon = dict((tag, cfd[tag].most_common(10)) for tag in cfd.conditions())\n",
    "    \n",
    "    #Clean out any punctuation issues\n",
    "    for tag,value in fourMostCommon.items():\n",
    "        index = 0 #Used to work around the dynamically changing size of value\n",
    "        while(index < len(value)):\n",
    "            pair = value[index] #One of the tuples\n",
    "            if (pair[0].isalpha() == False):\n",
    "                value.remove(pair)\n",
    "                index = 0 #Need to check all remaining values, so index is reset\n",
    "            elif (pair[0] == \"Mr\" or pair[0] == \"Mrs\" or pair[0] == \"Miss\" or pair[0] == \"Ms\" or pair[0] == \"A\" or\n",
    "                     pair[0] == \"St\"):\n",
    "                value.remove(pair)\n",
    "                index = 0 #Need to check all remaining values, so index is reset\n",
    "            else:\n",
    "                index = index + 1\n",
    "        fourMostCommon[tag] = value\n",
    "    \n",
    "    #Put together a list of all the counts and find the max\n",
    "    #listOfAllPropNouns\n",
    "    listOfAllCounts = list()\n",
    "    for tag,value in fourMostCommon.items():\n",
    "        for i in range(len(value)):\n",
    "            pair = value[i]\n",
    "            listOfAllCounts.append(pair[1])\n",
    "    greatest = 0\n",
    "    if (len(listOfAllCounts) > 0): #Account for overly used common proper nouns\n",
    "        greatest = max(listOfAllCounts)\n",
    "        \n",
    "    #Put together a list of all the proper nouns and find the proper noun corresponding to the max\n",
    "    listOfAllPNouns = list()\n",
    "    for tag,value in fourMostCommon.items():\n",
    "        for i in range(len(value)):\n",
    "            pair = value[i]\n",
    "            listOfAllPNouns.append(pair)\n",
    "    \n",
    "    bestPair = tuple() #The tuple that corresponds to the most frequently occuring proper noun\n",
    "    for i in range(len(listOfAllPNouns)):\n",
    "        pair = listOfAllPNouns[i]\n",
    "        if (pair[1] == greatest):\n",
    "            bestPair = pair\n",
    "    \n",
    "    return bestPair\n",
    "###########################End of Function############################################################\n",
    "\n",
    "#Get all the Authors to go through:\n",
    "dirList = list()\n",
    "for root, dirs, files in os.walk(\"../Processed\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if (name != \".ipynb_checkpoints\"):\n",
    "            dirList.append(name)\n",
    "\n",
    "#First create the dataFrame that will hold the results:\n",
    "finalDFDict = dict()\n",
    "finalDFDict = OrderedDict({\"authors\": dirList, \"CommonProperNouns\": set(['', '1', '2'])})\n",
    "finalDF = pd.DataFrame(finalDFDict)\n",
    "\n",
    "#Iterate through directory and process the data:\n",
    "for dirs in dirList:\n",
    "    propNounList = set() #A set of all the common proper nouns per author\n",
    "    for file in os.listdir(os.fsencode(dirs)):\n",
    "        fileName = os.fsencode(file).decode(\"utf-8\")\n",
    "        print(\"This book will be processed: \" + dirs + \" \" + str(fileName))\n",
    "\n",
    "        #Read in the raw data as a string\n",
    "        rawDataFile = open(dirs + \"/\"+ str(fileName), \"r\", encoding=\"utf-8\")\n",
    "        rawData = rawDataFile.read() #type(rawData) == string\n",
    "        rawDataLines = rawData.split(\"/n\") #Look, lists can be easier to deal with!\n",
    "        #print(\"Read the data for \" + str(fileName) + \"!\")\n",
    "        \n",
    "        #One thing that can be useful is removing stopwords, and the NLTK lib does this for you!\n",
    "        #Stopwords are super-common things like \"this\", \"the\", etc. and are usually removed to improve performance\n",
    "        processedText = remStopwordsFromStr(rawData)\n",
    "        processedText = remPuncFromStr(processedText)\n",
    "        #print(\"Stopword and punctuation processing for \" + str(fileName) + \" done!\")\n",
    "                    \n",
    "        #Okay, so now all of our text should have no stopwords\n",
    "        #Thus, let's get a list of proper nouns\n",
    "        \n",
    "        #First, we need to tokenize the text. Then we tag the words with their respective parts of speech:\n",
    "        tokenized_all_text = word_tokenize(processedText) #tokenize the text\n",
    "        list_of_tagged_words = nltk.pos_tag(tokenized_all_text) #adding POS Tags to tokenized words\n",
    "        set_pos  = (set(list_of_tagged_words)) # set of POS tags & words\n",
    "        #print(\"Tagged all the words in file \" + str(fileName) + \"!\")\n",
    "        \n",
    "        #We need to get just the proper nouns and find the n most common ones:\n",
    "        properNouns = [\"NNP\",\"NNPS\"] # POS tags of proper nouns\n",
    "        listOfPNouns = list(map(lambda tuple_2 : tuple_2[0], filter(lambda tuple_2 : tuple_2[1] in  properNouns, set_pos)))\n",
    "\n",
    "        #Now we create a new dataframe:\n",
    "        result = mostFreqProperNoun(\"NNP\", list_of_tagged_words)\n",
    "        propNounList.add(result[0])\n",
    "        finalDF.at[dirList.index(dirs), \"CommonProperNouns\"] = propNounList\n",
    "    \n",
    "    #Friendly message to the user:\n",
    "    print(\"Finished with \" + str(dirs) + \"'s books!\")\n",
    "\n",
    "#Push data to a CSV to read from later:\n",
    "finalDF.to_csv(path_or_buf=os.getcwd() + \"\\\\features.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "This book will be processed: Dickens Bleak_House.txt\n",
      "This book will be processed: Dickens David_Copperfield.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukund\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book will be processed: Dickens Dombey_and_Son.txt\n",
      "This book will be processed: Dickens Great_Expectations.txt\n",
      "This book will be processed: Dickens Little_Dorrit.txt\n",
      "This book will be processed: Dickens Nicholas_Nickleby.txt\n",
      "This book will be processed: Dickens Oliver_Twist.txt\n",
      "This book will be processed: Dickens Our_Mutual_Friend.txt\n",
      "This book will be processed: Dickens The_Letters_of_Charles_Dickens.txt\n",
      "This book will be processed: Dickens The_Pickwick_Papers.txt\n",
      "Finished with Dickens's books!\n",
      "This book will be processed: Tolstoy Anna_Karenina.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukund\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:40: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book will be processed: Tolstoy Kingdom_of_God_is_Within_You.txt\n",
      "This book will be processed: Tolstoy Sevastopol.txt\n",
      "This book will be processed: Tolstoy The_Cossacks.txt\n",
      "This book will be processed: Tolstoy The_Kreutzer_Sonata_and_Other_Stories.txt\n",
      "This book will be processed: Tolstoy The_Resurrection.txt\n",
      "This book will be processed: Tolstoy War_And_Peace.txt\n",
      "Finished with Tolstoy's books!\n",
      "This book will be processed: Twain Adventures_of_Huckleberry_Finn.txt\n",
      "This book will be processed: Twain A_Connecticut_Yankee_in_King_Arthur's_Court.txt\n",
      "This book will be processed: Twain Life_on_the_Mississippi.txt\n",
      "This book will be processed: Twain Roughing_It.txt\n",
      "This book will be processed: Twain The_Adventures_Of_Tom_Sawyer.txt\n",
      "This book will be processed: Twain The_Innocents_Abroad.txt\n",
      "This book will be processed: Twain The_Prince_And_The_Pauper.txt\n",
      "This book will be processed: Twain The_Tragedy_of_Pudd'nhead_Wilson.txt\n",
      "Finished with Twain's books!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re #Regular expression operations\n",
    "import string\n",
    "import pandas as pd #To create a dataframe of data\n",
    "#NLTK is an interesting library that was used in a Kaggle kernel and helps with a bunch of NLP stuff\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords #for removing stopwords\n",
    "\n",
    "#Get all the Authors to go through:\n",
    "dirList = list()\n",
    "for root, dirs, files in os.walk(\"../Processed\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if (name != \".ipynb_checkpoints\"):\n",
    "            dirList.append(name)\n",
    "\n",
    "#Read in the CSV:\n",
    "featuresDF = pd.read_csv(\"features.csv\", index_col=0, header=0)\n",
    "propNounDict = list() #A set of all the common proper nouns per author\n",
    "\n",
    "#Iterate through directory and process the data:\n",
    "for dirs in dirList:\n",
    "    totalCount = 0\n",
    "    for file in os.listdir(os.fsencode(dirs)):\n",
    "        fileName = os.fsencode(file).decode(\"utf-8\")\n",
    "        print(\"This book will be processed: \" + dirs + \" \" + str(fileName))\n",
    "        \n",
    "        rawDataFile = open(dirs + \"/\"+ str(fileName), \"r\", encoding=\"utf-8\")\n",
    "        rawData = rawDataFile.read() #type(rawData) == string\n",
    "        \n",
    "        #Count # of stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        count = len([w for w in str(rawData).lower().split() if w in stop_words])\n",
    "        \n",
    "        totalCount = count + totalCount\n",
    "        \n",
    "    featuresDF.at[dirList.index(dirs), \"avgStopWordCountPerBook\"] = (totalCount/len(os.listdir(os.fsencode(dirs))))\n",
    "    #Friendly message to the user:\n",
    "    print(\"Finished with \" + str(dirs) + \"'s books!\")\n",
    "\n",
    "#Push data to a CSV to read from later:\n",
    "featuresDF.to_csv(path_or_buf=os.getcwd() + \"\\\\features.csv\", header=True, index=True)\n",
    "featuresDF.to_csv(path_or_buf=os.getcwd() + \"\\\\numFeatures.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NGram Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "This book will be processed: Dickens Bleak_House.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukund\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book will be processed: Dickens David_Copperfield.txt\n",
      "This book will be processed: Dickens Dombey_and_Son.txt\n",
      "This book will be processed: Dickens Great_Expectations.txt\n",
      "This book will be processed: Dickens Little_Dorrit.txt\n",
      "This book will be processed: Dickens Nicholas_Nickleby.txt\n",
      "This book will be processed: Dickens Oliver_Twist.txt\n",
      "This book will be processed: Dickens Our_Mutual_Friend.txt\n",
      "This book will be processed: Dickens The_Letters_of_Charles_Dickens.txt\n",
      "This book will be processed: Dickens The_Pickwick_Papers.txt\n",
      "Finished with Dickens's books!\n",
      "This book will be processed: Tolstoy Anna_Karenina.txt\n",
      "This book will be processed: Tolstoy Kingdom_of_God_is_Within_You.txt\n",
      "This book will be processed: Tolstoy Sevastopol.txt\n",
      "This book will be processed: Tolstoy The_Cossacks.txt\n",
      "This book will be processed: Tolstoy The_Kreutzer_Sonata_and_Other_Stories.txt\n",
      "This book will be processed: Tolstoy The_Resurrection.txt\n",
      "This book will be processed: Tolstoy War_And_Peace.txt\n",
      "Finished with Tolstoy's books!\n",
      "This book will be processed: Twain Adventures_of_Huckleberry_Finn.txt\n",
      "This book will be processed: Twain A_Connecticut_Yankee_in_King_Arthur's_Court.txt\n",
      "This book will be processed: Twain Life_on_the_Mississippi.txt\n",
      "This book will be processed: Twain Roughing_It.txt\n",
      "This book will be processed: Twain The_Adventures_Of_Tom_Sawyer.txt\n",
      "This book will be processed: Twain The_Innocents_Abroad.txt\n",
      "This book will be processed: Twain The_Prince_And_The_Pauper.txt\n",
      "This book will be processed: Twain The_Tragedy_of_Pudd'nhead_Wilson.txt\n",
      "Finished with Twain's books!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re #Regular expression operations\n",
    "import string\n",
    "import pandas as pd #To create a dataframe of data\n",
    "import operator\n",
    "#NLTK is an interesting library that was used in a Kaggle kernel and helps with a bunch of NLP stuff\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords #for removing stopwords\n",
    "from collections import Counter\n",
    "\n",
    "#######################################Remove stopwords from string##############################\n",
    "def remStopwordsFromStr(string1):\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*') #compiling all stopwords.\n",
    "    string2 = pattern.sub('', string1) #replacing the occurrences of stopwords in string1\n",
    "    return string2\n",
    "###########################End of Function#######################################################\n",
    "\n",
    "###########################Function for removing punctuations from string########################\n",
    "def remPuncFromStr(string1):\n",
    "    translation_table = dict.fromkeys(map(ord, string.punctuation), ' ') #creating dictionary of punc & None\n",
    "    string2 = string1.translate(translation_table) #apply punctuation removal\n",
    "    return string2\n",
    "############################End of Function###############################################\n",
    "\n",
    "##################################Build ngrams from the data#####################################\n",
    "def ngramListGenerator(string1,count_of_words_in_ngram):\n",
    "    #string1 = string1.lower()\n",
    "    string1 = string1.replace('.','. ')\n",
    "    all_grams = ngrams(string1.split(), count_of_words_in_ngram)\n",
    "    grams_list = []\n",
    "    for grams in all_grams:\n",
    "        grams_list.append(grams)\n",
    "    return(grams_list)\n",
    "##################################End of Function################################################\n",
    "\n",
    "#Get all the Authors to go through:\n",
    "dirList = list()\n",
    "for root, dirs, files in os.walk(\"../Processed\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if (name != \".ipynb_checkpoints\"):\n",
    "            dirList.append(name)\n",
    "\n",
    "#Read in the CSV:\n",
    "featuresDF = pd.read_csv(\"features.csv\", index_col=0, header=0)\n",
    "featuresDF[\"CommonNGrams\"] = \"\"\n",
    "\n",
    "#Iterate through directory and process the data:\n",
    "for dirs in dirList:\n",
    "    totalCount = 0\n",
    "    listOfNGramsPerAuthor = set()\n",
    "    for file in os.listdir(os.fsencode(dirs)):\n",
    "        fileName = os.fsencode(file).decode(\"utf-8\")\n",
    "        print(\"This book will be processed: \" + dirs + \" \" + str(fileName))\n",
    "        \n",
    "        rawDataFile = open(dirs + \"/\"+ str(fileName), \"r\", encoding=\"utf-8\")\n",
    "        rawData = rawDataFile.read() #type(rawData) == string\n",
    "        \n",
    "        #Remove punctuation and stopwords:\n",
    "        processedText = remStopwordsFromStr(rawData)\n",
    "        processedText = remPuncFromStr(processedText)\n",
    "        \n",
    "        #Get bigrams of text:\n",
    "        ngramList = ngramListGenerator(processedText, 2)\n",
    "        \n",
    "        #Getting count for every bigram:\n",
    "        ngramCounts = Counter(ngramList)\n",
    "\n",
    "        #Getting top 10 bigram as per highest count:\n",
    "        sortedNGram = dict(sorted(ngramCounts.items(), key=operator.itemgetter(1),reverse=True)[:10])\n",
    "        \n",
    "        #Filter out strange nGrams:\n",
    "        processedNGrams = {k: v for k, v in sortedNGram.items() if (k[0].isalpha() != False and k[1].isalpha() != False)}\n",
    "        \n",
    "        #Store the nGrams in a list\n",
    "        for i in processedNGrams:\n",
    "            listOfNGramsPerAuthor.add(i)\n",
    "    \n",
    "    #Now add the list to the features:\n",
    "    featuresDF.at[dirList.index(dirs), \"CommonNGrams\"] = listOfNGramsPerAuthor\n",
    "        \n",
    "    #Friendly message to the user:\n",
    "    print(\"Finished with \" + str(dirs) + \"'s books!\")\n",
    "\n",
    "#Push data to a CSV to read from later:\n",
    "featuresDF.to_csv(path_or_buf=os.getcwd() + \"\\\\features.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Finished with Dickens!\n",
      "Finished with Tolstoy!\n",
      "Finished with Twain!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re #Regular expression operations\n",
    "import string\n",
    "import pandas as pd #To create a dataframe of data\n",
    "import operator\n",
    "#NLTK is an interesting library that was used in a Kaggle kernel and helps with a bunch of NLP stuff\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords #for removing stopwords\n",
    "from collections import Counter\n",
    "\n",
    "#Get all the Authors to go through:\n",
    "dirList = list()\n",
    "for root, dirs, files in os.walk(\"../Processed\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if (name != \".ipynb_checkpoints\"):\n",
    "            dirList.append(name)\n",
    "\n",
    "#Read in the CSV:\n",
    "featuresDF = pd.read_csv(\"features.csv\", index_col=0, header=0)\n",
    "featuresDF[\"BagOfWords\"] = \"\"\n",
    "\n",
    "#Iterate through each set of NGrams and create the bag of words:\n",
    "for i in range(len(featuresDF[\"CommonNGrams\"])):\n",
    "    bagOfWords = set() #The final bag of words for the author\n",
    "    \n",
    "    #Pull the list of NGrams for author x:\n",
    "    listOfNGrams = featuresDF.at[i, \"CommonNGrams\"]\n",
    "    listOfNGrams = listOfNGrams.split(\",\")\n",
    "    \n",
    "    for j in range(len(listOfNGrams)):\n",
    "        string = listOfNGrams[j]\n",
    "        if (string.find(\"(\") != -1):\n",
    "            string = string[string.find(\"(\")+1:]\n",
    "        elif (string.find(\")\") != -1):\n",
    "            string = string[:string.find(\")\")]\n",
    "        listOfNGrams[j] = string\n",
    "    \n",
    "    #Now get every unique word in that list\n",
    "    for k in range(len(listOfNGrams)):\n",
    "        bagOfWords.add(listOfNGrams[k])\n",
    "    \n",
    "    #Now add the bag of words to the features:\n",
    "    featuresDF.at[i, \"BagOfWords\"] = bagOfWords\n",
    "        \n",
    "    #Friendly message to the user: (Plural - authors)\n",
    "    print(\"Finished with \" + str(featuresDF.at[i,\"authors\"]) + \"!\")\n",
    "\n",
    "#Push data to a CSV to read from later:\n",
    "featuresDF.to_csv(path_or_buf=os.getcwd() + \"\\\\features.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mukund\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "This book will be processed: Dickens Bleak_House.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mukund\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: The Windows bytes API has been deprecated, use Unicode filenames instead\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-97c847fee5b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m#Count frequencies for common POS types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mposList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mfvsSyntax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposList\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#Normalise by dividing each row by number of tokens in the chapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-97c847fee5b9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m#Count frequencies for common POS types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mposList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mfvsSyntax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposList\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#Normalise by dividing each row by number of tokens in the chapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-97c847fee5b9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m#Count frequencies for common POS types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mposList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'JJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NNS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mfvsSyntax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mposList\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#Normalise by dividing each row by number of tokens in the chapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re #Regular expression operations\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd #To create a dataframe of data\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#NLTK is an interesting library that was used in a Kaggle kernel and helps with a bunch of NLP stuff\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords #for removing stopwords\n",
    "from collections import OrderedDict\n",
    "from _collections_abc import dict_keys\n",
    "\n",
    "######################################Get part of speech for each token in the chapter##########################\n",
    "def tokenToPos(ch):\n",
    "            tokens = nltk.word_tokenize(ch)\n",
    "            return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "######################################End of Function###########################################################\n",
    "\n",
    "#Get all the Authors to go through:\n",
    "dirList = list()\n",
    "for root, dirs, files in os.walk(\"../Processed\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if (name != \".ipynb_checkpoints\"):\n",
    "            dirList.append(name)\n",
    "\n",
    "#First create the dataFrame that will hold the results:\n",
    "finalDFDict = dict()\n",
    "finalDFDict = OrderedDict({\"authors\": dirList, \"avgStopWordPerSent\": set(['', '1', '2']),\n",
    "                               \"avgWordsPerSent\":set(['', '1', '2']), \"sentLengthVar\":set(['', '1', '2']),\n",
    "                               \"lexicalDiv\":set(['', '1', '2']), })\n",
    "numFeaturesDF = pd.DataFrame(finalDFDict)\n",
    "\n",
    "#Iterate through directory and process the data:\n",
    "for dirs in dirList:\n",
    "    totalStopWordCount = 0\n",
    "    totalAvgNumWords = 0\n",
    "    totalSentLengthVar = 0\n",
    "    totalLexicalDiv = 0\n",
    "    totalCommaPerSent = 0\n",
    "    totalSemiPerSent = 0\n",
    "    totalColonsPerSent = 0\n",
    "    for file in os.listdir(os.fsencode(dirs)):\n",
    "        fileName = os.fsencode(file).decode(\"utf-8\")\n",
    "        print(\"This book will be processed: \" + dirs + \" \" + str(fileName))\n",
    "        \n",
    "        rawDataFile = open(dirs + \"/\"+ str(fileName), \"r\", encoding=\"utf-8\")\n",
    "        rawData = rawDataFile.read() #type(rawData) == string\n",
    "        words = word_tokenize(rawData)\n",
    "        allSentences = sent_tokenize(rawData)\n",
    "        \n",
    "        #Number of words in each sentence\n",
    "        wordsPerSentence = np.array([len(word_tokenize(sent)) for sent in allSentences])\n",
    "        \n",
    "        #Avg number of words per sentence\n",
    "        avgNumWord = wordsPerSentence.mean()\n",
    "        totalAvgNumWords = avgNumWord + totalAvgNumWords\n",
    "        \n",
    "        #Sentence length variation\n",
    "        sentLengthVar = wordsPerSentence.std()\n",
    "        totalSentLengthVar = sentLengthVar + totalSentLengthVar\n",
    "        \n",
    "        #Lexical diversity\n",
    "        lexicalDiv = len(set(word_tokenize(sent))) / float(len(word_tokenize(sent)))\n",
    "        totalLexicalDiv = lexicalDiv + totalLexicalDiv\n",
    "        \n",
    "        #Count num of stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stopWordCount = len([w for w in str(rawData).lower().split() if w in stop_words])\n",
    "        totalStopWordCount = stopWordCount + totalStopWordCount\n",
    "        \n",
    "        #Commas per sentence\n",
    "        commaPerSent = words.count(',') / float(len(allSentences))\n",
    "        totalCommaPerSent = commaPerSent + totalCommaPerSent\n",
    "        \n",
    "        #Semicolons per sentence\n",
    "        semiPerSent = words.count(';') / float(len(allSentences))\n",
    "        totalSemiPerSent = semiPerSent + totalSemiPerSent\n",
    "        \n",
    "        #Colons per sentence\n",
    "        colonsPerSent = words.count(':') / float(len(allSentences))\n",
    "        totalColonsPerSent = colonsPerSent + totalColonsPerSent\n",
    "        \n",
    "        #Bag of Words features:\n",
    "        #Get most common words in the whole book\n",
    "        #numTopWords = 10\n",
    "        #allTokens = word_tokenize(rawData)\n",
    "        #fDist = nltk.FreqDist(allTokens)\n",
    "        #vocab = list(fDist.keys())[:numTopWords]\n",
    " \n",
    "        #Use sklearn to create the bag for words feature vector for each chapter\n",
    "        #vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=word_tokenize)\n",
    "        #fvsBow = vectorizer.fit_transform(rawDataFile.read().replace('\\n', ' ')).toarray().astype(np.float64)\n",
    "\n",
    "        #Normalise by dividing each row by its Euclidean norm\n",
    "        #fvsBow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvsBow)]\n",
    "        #print(fvsBow)\n",
    "        \n",
    "        #\n",
    "        #Get POS for each token in each chapter\n",
    "        bookPos = tokenToPos(rawData)\n",
    " \n",
    "        #Count frequencies for common POS types\n",
    "        posList = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "        fvsSyntax = np.array([[rawData.count(pos) for pos in posList] for i in list(rawData)]).astype(np.float64)\n",
    " \n",
    "        #Normalise by dividing each row by number of tokens in the chapter\n",
    "        fvsSyntax /= np.c_[np.array([len(rawData)])]\n",
    "        print(fvsSyntax)\n",
    "        \n",
    "    numFeaturesDF.at[dirList.index(dirs), \"avgStopWordCountPerBook\"] = (totalCount/len(os.listdir(os.fsencode(dirs))))\n",
    "    #Friendly message to the user:\n",
    "    print(\"Finished with \" + str(dirs) + \"'s books!\")\n",
    "\n",
    "#Push data to a CSV to read from later:\n",
    "numFeaturesDF.to_csv(path_or_buf=os.getcwd() + \"\\\\numFeatures.csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
