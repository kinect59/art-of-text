So we finished 6:45 right? 6:15. Yeah, thats right. That's what we're going to do on Monday. We're going to go till we pass out or we're all done, alright? So Monday December 11th, you have to present and be there. We were aiming for something like 10 minutes, but I want to sit down and see how many teams we have and try to keep it reasonable for everyone. We'll bring pizza, and hopefully survive. It should be fun. And you know we'll hopefully survive. I guess if we're 60 people roughly, over four, is fifteen times ten. two and a half. Eight days? It's a little long. So we're aiming for something like that, like ten minutes something. But there's also overhead in switching, so we're aiming to maybe put everything in one laptop. We'll try to put everything in one laptop so we don't have those delays. Of course. 

Alright. Good. So let's talk today. Just a few words, so projects. Please start seriously thinking about projects. Hopefully I have asked some of graduate students and TAs and myself are helping on different projects. If you, don't be afraid to go into a more fancy direction like the one I'm going to talk about today. Which is pretty awesome. I don't know how you can use it in your project, but you can be quite flexible. You have a lot of time, and that's the good thing. I know most of you will do 90 percent of your project in the last three days, and I hope not. But that's what typically happens. So.

Alright. Do we all know what is transfered learning? Do you know what that is? that is a very useful thing. So. Let's briefly talk about that because you may use it for your project. Transfered learning is, say you want to do some image processing task. Let's say recognizing different types of fruit or something. And you don't have enough training set to build your fruit classifier from a million labeled fruit. It's easy to find a million images, but it's not easy to find a million labeled images. Okay? So. What you do is you download a pretrained neural network that has been trained on a bigger class of problems, like ImageNet. Which is a thousand label neuralnet you can download trained from Google. Then you kill the last layer. So i'm just showing this here in the slide. I'll just show you this idea quickly and then we'll move to generators. So people have trained have trained this massive network. So inceptionv3 is a network that has been trained on images. 1 terabyte dataset of images. Which has a thousand different classes. So you can download these weights and here it has and uses a thousand different labels and it's usually correct. For example this is a cat. Then what you do is you okay so, good, so what you do is take the last slice it here. And you look at this and look at these wires at somepoint here. These wires are very good features and now lets say you want to classify you fruit classifer with only 100 labeled fruit. You're just going to make a logistic regression and make a classifier but instead of looking at pixels you're going to use these things as features. So that's the idea of transfered learning. And you can do this for example, word2vec, which takes words. Did we talk about word2vec? So you can download word2vec and take a vector. And it will have a conceptual distances. So you can use that as a feature, is you have a text now. If you have a text, you can word2vec every word and now you have some sort of embedding of text as a bunch of points in conceptual space. And you will think that text about history would be some point history would be some point here and that text about sport would be somewhere else. So you can use pre trained things to do your tasks which allows you to integrate more knowledge. So it's extremely useful for the types of projects you guys want to do. Because you're not google and you don't have millions of data that you can use. 

Alright so. That’s the. What i’m saying these are excellent image features and you can train. There’s a google tutorial for tensorflow that can do just that, and you can trivially play with it. 

Now let’s talk about generators. So this lecture is about generative models. Ganns and VAEs. I think I have a. I don’t have a clicker. Too bad. Okay so. Generative models are trying to solve the following problem. We want to discriminators takes pixels to tell if it’s an image of cat or dog. And here we’re going to think of probability distribution that are extremely complication. Like the image the pixels of an image of a face. Okay? So I want to think for a second. Think of a vector of an image, alright? So you have an image like this alright? So it's 64 pixels by 64 pixels and it’s by 3 because it's RGB. So it's 13,000 numbers so you think you can stack these 13 thousand numbers in a big vector here and blah blah blah. 13 thousand numbers. Now. You can think of the space of 13 thousand dimensional vectors, like you can think of 3 dimensional space, you can think of 13 thousand space. So it’s some space here. You randomly think of a point here. It’s going to look like some bunch of random noise. Okay? So what i’m trying to communicate is that you can think about the set of images that human would call human faces, and you can think that this thing creates some sort of weird set here. This set is extremely complicated. Right? But if we had a way, a machine, to somehow randomly pick a point from this set. We would be getting a random human looking face here, right? So that’s what we're trying to do. We’re trying to think of a probability distribution and we want to learn a extremely complicated distribution on theis space like the images corresponding to faces or images corresponding to bedrooms or images corresponding to cats. Or another corresponding to natural images. Which would be a bigger set that would include faces, right? Remarkably, we can sort of do that today, where last year we could not do that. There has been remarkable progress in solving this problem in the last two years, these are called generative models. I would like to push a button and sample randomly from this manifold and think of that as sampling a random face out of all possible faces in the universe. This is unsupervised problem, because there's no labels in this problem. The only input given to us is a bunch of faces that we’re going to train. You can download a data set called celeb a that have been nicely cropped to 64 by 64. So this is going to be our dataset and we want to train a neural network that can sample from this crazy distribution. We’re going to talk about two ways to do that. Were going to talk about maximizing the likelihood of the data. This is the first thing i want you to keep in mind. My first goal is the explain what the hell that means. And the second problem is instead of standard training, there’s a new idea that has only existed for three or four years. And we’re going to talk about both of these ideas and train neural networks. What is standard training? Maximum likelihood training. So we're going to talk about a silly neural network here.
